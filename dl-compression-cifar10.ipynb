{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NquqqwdT3aSn",
    "outputId": "97eb7384-d968-4108-ab6a-a3a9d8ff9caf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 16:17:12.075252: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-10 16:17:12.101243: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-10 16:17:12.101269: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-10 16:17:12.101293: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-10 16:17:12.106780: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-10 16:17:12.107533: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-10 16:17:12.791343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)\n",
    "print(len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdA356Bj3fH0",
    "outputId": "f1603412-6f3b-4973-9d59-d9b02673e811"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 16:17:17.343554: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1228800000 exceeds 10% of free system memory.\n",
      "2023-11-10 16:17:18.350324: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 245760000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "#IMPORTAÇÃO E NORRMALIZAÇÃO\n",
    "(x_train, y_train), (x_test,y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "#x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "#x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype(float)/255\n",
    "x_test = x_test.astype(float)/255\n",
    "\n",
    "#y_train = to_categorical(y_train, num_classes = 10)\n",
    "#y_test = to_categorical(y_test, num_classes = 10)\n",
    "\n",
    "#CRIAR DATASET\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(50000).batch(64)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YImlHEfh5ZS",
    "outputId": "33a8eed3-f06e-42b7-876a-8ede192d8c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-USIOZVn3jbd",
    "outputId": "24fab0dd-5f4c-4081-cd82-98edd4bebef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_13 (Conv2D)          (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 16, 16, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 8, 8, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 4, 4, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 2, 2, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 1, 1, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33638218 (128.32 MB)\n",
      "Trainable params: 33638218 (128.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Block 1\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "#model.add(BatchNormalization(name=\"bn1\"))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(BatchNormalization(name=\"bn2\"))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "#model.add(Dropout(0.25))  # Dropout layer after the block\n",
    "\n",
    "# Block 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(BatchNormalization(name=\"bn3\"))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(BatchNormalization(name=\"bn4\"))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "#model.add(Dropout(0.25))  # Dropout layer after the block\n",
    "\n",
    "# Block 3\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(BatchNormalization(name=\"bn5\"))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(BatchNormalization(name=\"bn6\"))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(BatchNormalization(name=\"bn7\"))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "#model.add(Dropout(0.25))  # Dropout layer after the block\n",
    "\n",
    "# Block 4\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(BatchNormalization(name=\"bn8\"))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(BatchNormalization(name=\"bn9\"))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(BatchNormalization(name=\"bn10\"))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "#model.add(Dropout(0.25))  # Dropout layer after the block\n",
    "\n",
    "# Block 5\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(BatchNormalization(name=\"bn11\"))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(BatchNormalization(name=\"bn12\"))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(BatchNormalization(name=\"bn13\"))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "#model.add(Dropout(0.25))  # Dropout layer after the block\n",
    "\n",
    "# Fully Connected Layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))  # 10 classes for CIFAR-10\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "for layer in model.trainable_variables:\n",
    "    #print(layer.name)\n",
    "    if 'bias' in layer.name:\n",
    "        new_bias = tf.cast(tf.where(tf.abs(layer) >= 0, 0.1, 0.1), tf.float32)\n",
    "        layer.assign(new_bias)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Z6nXl6SRjKQW"
   },
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9)\n",
    "acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "test_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_accuracy = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FV9-3epGiwEv",
    "outputId": "d2301847-cd14-442f-d972-db31f77254d8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 1/50 \t Loss = 2.309 \t Train Acc = 10.230% \t Sparsity = 72.139% \t Test Loss = 2.304 \tTest Acc = 10.000%\n",
      "New test accuracy is 10.000%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 2/50 \t Loss = 2.309 \t Train Acc = 9.964% \t Sparsity = 72.139% \t Test Loss = 2.313 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 3/50 \t Loss = 2.308 \t Train Acc = 10.172% \t Sparsity = 72.139% \t Test Loss = 2.306 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 4/50 \t Loss = 2.307 \t Train Acc = 10.092% \t Sparsity = 72.139% \t Test Loss = 2.308 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 5/50 \t Loss = 2.307 \t Train Acc = 10.070% \t Sparsity = 72.139% \t Test Loss = 2.308 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 6/50 \t Loss = 2.306 \t Train Acc = 9.804% \t Sparsity = 72.139% \t Test Loss = 2.309 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 7/50 \t Loss = 2.306 \t Train Acc = 9.830% \t Sparsity = 72.139% \t Test Loss = 2.308 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 8/50 \t Loss = 2.305 \t Train Acc = 10.240% \t Sparsity = 72.139% \t Test Loss = 2.306 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 9/50 \t Loss = 2.306 \t Train Acc = 9.958% \t Sparsity = 72.139% \t Test Loss = 2.304 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 10/50 \t Loss = 2.306 \t Train Acc = 9.782% \t Sparsity = 72.139% \t Test Loss = 2.305 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 11/50 \t Loss = 2.305 \t Train Acc = 9.786% \t Sparsity = 72.139% \t Test Loss = 2.303 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 12/50 \t Loss = 2.305 \t Train Acc = 10.004% \t Sparsity = 72.139% \t Test Loss = 2.303 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 13/50 \t Loss = 2.305 \t Train Acc = 10.074% \t Sparsity = 72.139% \t Test Loss = 2.305 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 14/50 \t Loss = 2.304 \t Train Acc = 10.262% \t Sparsity = 72.139% \t Test Loss = 2.305 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 15/50 \t Loss = 2.305 \t Train Acc = 10.028% \t Sparsity = 72.139% \t Test Loss = 2.303 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 16/50 \t Loss = 2.304 \t Train Acc = 10.038% \t Sparsity = 72.139% \t Test Loss = 2.307 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 17/50 \t Loss = 2.304 \t Train Acc = 10.012% \t Sparsity = 72.139% \t Test Loss = 2.304 \tTest Acc = 14.490%\n",
      "New test accuracy is 14.490%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 18/50 \t Loss = 2.304 \t Train Acc = 10.090% \t Sparsity = 72.139% \t Test Loss = 2.303 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 19/50 \t Loss = 2.304 \t Train Acc = 10.058% \t Sparsity = 72.139% \t Test Loss = 2.303 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 20/50 \t Loss = 2.304 \t Train Acc = 10.212% \t Sparsity = 72.139% \t Test Loss = 2.303 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 21/50 \t Loss = 2.304 \t Train Acc = 9.980% \t Sparsity = 72.139% \t Test Loss = 2.304 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 22/50 \t Loss = 2.303 \t Train Acc = 10.302% \t Sparsity = 72.140% \t Test Loss = 2.303 \tTest Acc = 10.000%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 23/50 \t Loss = 2.297 \t Train Acc = 11.548% \t Sparsity = 72.141% \t Test Loss = 2.249 \tTest Acc = 18.410%\n",
      "New test accuracy is 18.410%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 24/50 \t Loss = 2.104 \t Train Acc = 19.394% \t Sparsity = 72.151% \t Test Loss = 2.078 \tTest Acc = 18.890%\n",
      "New test accuracy is 18.890%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 25/50 \t Loss = 2.018 \t Train Acc = 21.528% \t Sparsity = 72.159% \t Test Loss = 1.950 \tTest Acc = 23.970%\n",
      "New test accuracy is 23.970%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 26/50 \t Loss = 1.906 \t Train Acc = 25.038% \t Sparsity = 72.175% \t Test Loss = 1.932 \tTest Acc = 23.580%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 27/50 \t Loss = 1.834 \t Train Acc = 28.312% \t Sparsity = 72.194% \t Test Loss = 1.817 \tTest Acc = 30.180%\n",
      "New test accuracy is 30.180%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 28/50 \t Loss = 1.748 \t Train Acc = 31.912% \t Sparsity = 72.221% \t Test Loss = 1.763 \tTest Acc = 31.080%\n",
      "New test accuracy is 31.080%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 29/50 \t Loss = 1.658 \t Train Acc = 35.836% \t Sparsity = 72.258% \t Test Loss = 1.599 \tTest Acc = 38.670%\n",
      "New test accuracy is 38.670%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 30/50 \t Loss = 1.561 \t Train Acc = 40.186% \t Sparsity = 72.306% \t Test Loss = 1.556 \tTest Acc = 41.040%\n",
      "New test accuracy is 41.040%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 31/50 \t Loss = 1.447 \t Train Acc = 45.526% \t Sparsity = 72.367% \t Test Loss = 1.379 \tTest Acc = 48.870%\n",
      "New test accuracy is 48.870%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 32/50 \t Loss = 1.315 \t Train Acc = 50.808% \t Sparsity = 72.436% \t Test Loss = 1.324 \tTest Acc = 51.090%\n",
      "New test accuracy is 51.090%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 33/50 \t Loss = 1.223 \t Train Acc = 54.850% \t Sparsity = 72.507% \t Test Loss = 1.180 \tTest Acc = 56.560%\n",
      "New test accuracy is 56.560%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 34/50 \t Loss = 1.131 \t Train Acc = 58.534% \t Sparsity = 72.583% \t Test Loss = 1.186 \tTest Acc = 56.990%\n",
      "New test accuracy is 56.990%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 35/50 \t Loss = 1.044 \t Train Acc = 62.078% \t Sparsity = 72.662% \t Test Loss = 1.120 \tTest Acc = 59.300%\n",
      "New test accuracy is 59.300%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 36/50 \t Loss = 0.983 \t Train Acc = 64.334% \t Sparsity = 72.745% \t Test Loss = 0.983 \tTest Acc = 64.650%\n",
      "New test accuracy is 64.650%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 37/50 \t Loss = 0.906 \t Train Acc = 67.068% \t Sparsity = 72.834% \t Test Loss = 0.953 \tTest Acc = 66.120%\n",
      "New test accuracy is 66.120%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 38/50 \t Loss = 0.834 \t Train Acc = 69.868% \t Sparsity = 72.928% \t Test Loss = 0.920 \tTest Acc = 67.100%\n",
      "New test accuracy is 67.100%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 39/50 \t Loss = 0.763 \t Train Acc = 72.530% \t Sparsity = 73.023% \t Test Loss = 1.008 \tTest Acc = 65.630%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 40/50 \t Loss = 0.703 \t Train Acc = 74.920% \t Sparsity = 73.117% \t Test Loss = 0.825 \tTest Acc = 70.940%\n",
      "New test accuracy is 70.940%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 41/50 \t Loss = 0.632 \t Train Acc = 77.282% \t Sparsity = 73.219% \t Test Loss = 0.802 \tTest Acc = 72.270%\n",
      "New test accuracy is 72.270%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 42/50 \t Loss = 0.571 \t Train Acc = 79.562% \t Sparsity = 73.324% \t Test Loss = 0.877 \tTest Acc = 70.550%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 43/50 \t Loss = 0.510 \t Train Acc = 81.998% \t Sparsity = 73.429% \t Test Loss = 0.835 \tTest Acc = 72.480%\n",
      "New test accuracy is 72.480%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 44/50 \t Loss = 0.457 \t Train Acc = 83.870% \t Sparsity = 73.536% \t Test Loss = 0.821 \tTest Acc = 73.570%\n",
      "New test accuracy is 73.570%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 45/50 \t Loss = 0.396 \t Train Acc = 86.040% \t Sparsity = 73.639% \t Test Loss = 0.838 \tTest Acc = 73.730%\n",
      "New test accuracy is 73.730%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 46/50 \t Loss = 0.347 \t Train Acc = 87.718% \t Sparsity = 73.740% \t Test Loss = 0.907 \tTest Acc = 72.430%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 47/50 \t Loss = 0.306 \t Train Acc = 89.258% \t Sparsity = 73.838% \t Test Loss = 0.888 \tTest Acc = 73.630%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 48/50 \t Loss = 0.263 \t Train Acc = 90.670% \t Sparsity = 73.927% \t Test Loss = 0.946 \tTest Acc = 73.980%\n",
      "New test accuracy is 73.980%, Model saved\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 49/50 \t Loss = 0.232 \t Train Acc = 91.906% \t Sparsity = 74.013% \t Test Loss = 0.975 \tTest Acc = 73.150%\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "Epoch 50/50 \t Loss = 0.206 \t Train Acc = 92.712% \t Sparsity = 74.094% \t Test Loss = 1.064 \tTest Acc = 73.240%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "alpha = 1.25\n",
    "n_bits = 0\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "model_sparsity = np.array([])\n",
    "model_train_loss = np.array([])\n",
    "model_train_acc = np.array([])\n",
    "model_test_loss = np.array([])\n",
    "model_test_acc = np.array([])\n",
    "sparsity = 0\n",
    "v = [0]*len(model.trainable_weights)\n",
    "\n",
    "last_test_acc = 0\n",
    "model_name = \"model_32_125.h5\"\n",
    "\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    #TODO: Quantizition of gammas and betas of batch_normalization layer\n",
    "    for epoch in range(epochs):\n",
    "        # Iterate over the batches of the dataset.\n",
    "        loss_batch = np.array([])\n",
    "        loss_test_batch = np.array([])\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
    "            if step % 50 ==0:\n",
    "              print(step)\n",
    "            n_zeros = 0\n",
    "            size = 0\n",
    "            #pruning\n",
    "            if alpha > 0:\n",
    "                bk = []\n",
    "                for layer_weights in model.trainable_variables:\n",
    "                    if 'bn' in layer_weights.name:\n",
    "                        bk.append(-1)\n",
    "                    else:\n",
    "                        #flatten weights\n",
    "                        f_weights = tf.reshape(layer_weights,[-1])\n",
    "                        #get standard deviation of each layer\n",
    "                        lim = alpha*tf.math.reduce_std(f_weights)\n",
    "                        bk.append(lim)\n",
    "                        #create a mask\n",
    "                        mask = tf.cast(tf.where(tf.abs(layer_weights)>lim,1,0), tf.float32)\n",
    "                        #assign pruned weights to the layer\n",
    "                        layer_weights.assign(tf.math.multiply(layer_weights,mask))\n",
    "                        #check sparsity\n",
    "                        flat_array = np.array((tf.reshape(mask,[-1])))\n",
    "                        n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
    "                        size += flat_array.shape[0]\n",
    "                        sparsity = n_zeros*100/size\n",
    "            else:\n",
    "                bk = [0] * len(model.trainable_weights)\n",
    "\n",
    "            #Cópia do modelo\n",
    "            if n_bits > 0 and alpha > 0:\n",
    "                model_copy = keras.models.clone_model(model)\n",
    "                model_copy.set_weights(model.get_weights())\n",
    "\n",
    "            #Quantização\n",
    "            if n_bits > 0 and alpha > 0:\n",
    "                for i, layer_weights in enumerate(model.trainable_variables):\n",
    "                    if 'bn' in layer_weights.name:\n",
    "                        pass\n",
    "                    else:\n",
    "                        qk_line = (tf.reduce_max(tf.math.abs(layer_weights)) - bk[i]) / (2 ** (n_bits - 1) - 1)\n",
    "                        ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
    "                        layer_weights.assign(ck)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = model(x_batch_train, training=True)\n",
    "                loss = loss_fn(y_batch_train, pred)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            #if step==0 and epoch==0:   \n",
    "                #print(\"Created Tensor of velocity\")\n",
    "                #v = np.zeros_like(grads)\n",
    "            if n_bits > 0 and alpha > 0:\n",
    "                for i, (layer_weights, copied_weights) in enumerate(zip(model.trainable_variables, model_copy.trainable_variables)):\n",
    "                    #TODO: Add momentum and velocity\n",
    "                    if step ==0 and epoch ==0:\n",
    "                        v[i]=grads[i]*0\n",
    "                    grads[i] = grads[i] * learning_rate\n",
    "                    #Get value of velocity\n",
    "                    \n",
    "                    v[i] = tf.math.subtract(momentum*v[i], grads[i])\n",
    "                    # WEIGHT UPDATE\n",
    "                    layer_weights.assign(tf.math.add(copied_weights, v[i]))\n",
    "            else:\n",
    "                for i, layer_weights in enumerate(model.trainable_variables):\n",
    "                    if step ==0 and epoch ==0:\n",
    "                        v[i]=grads[i]*0\n",
    "                    grads[i] = grads[i] * learning_rate\n",
    "                    v[i] = tf.math.subtract(momentum*v[i], grads[i])\n",
    "                    #WEIGHT UPDATE\n",
    "                    layer_weights.assign(tf.math.add(layer_weights, v[i]))\n",
    "            predictions = tf.argmax(pred, axis=1, output_type=tf.int32)\n",
    "            train_accuracy.update_state(y_batch_train,predictions)\n",
    "\n",
    "            #predictions = tf.argmax(pred, axis=1, output_type=tf.int32)\n",
    "            #acc = train_accuracy(y_batch_train, predictions)\n",
    "\n",
    "        #keep track of loss by batch\n",
    "            loss_batch = np.append(loss_batch, loss)\n",
    "        acc = train_accuracy.result()\n",
    "\n",
    "        #reset states\n",
    "        train_accuracy.reset_states()\n",
    "        #mean of loss by epoch\n",
    "        model_train_loss = np.append(model_train_loss, np.mean(loss_batch))\n",
    "        #train accuracy by epoch\n",
    "        model_train_acc = np.append(model_train_acc, acc*100)\n",
    "        #sparsity by epoch\n",
    "        model_sparsity = np.append(model_sparsity, sparsity)\n",
    "\n",
    "        if alpha > 0:\n",
    "            bk = []\n",
    "            for layer_weights in model.trainable_variables:\n",
    "                if 'bn' in layer_weights.name:\n",
    "                    bk.append(-1)\n",
    "                else:\n",
    "                    #flatten weights\n",
    "                    f_weights = tf.reshape(layer_weights,[-1])\n",
    "                    #get standard deviation of each layer\n",
    "                    lim = alpha*tf.math.reduce_std(f_weights)\n",
    "                    bk.append(lim)\n",
    "                    #create a mask\n",
    "                    mask = tf.cast(tf.where(tf.abs(layer_weights)>lim,1,0), tf.float32)\n",
    "                    #assign pruned weights to the layer\n",
    "                    layer_weights.assign(tf.math.multiply(layer_weights,mask))\n",
    "                    #check sparsity\n",
    "                    flat_array = np.array((tf.reshape(mask,[-1])))\n",
    "                    n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
    "                    size += flat_array.shape[0]\n",
    "                    sparsity = n_zeros*100/size\n",
    "        else:\n",
    "            bk = [0] * len(model.trainable_weights)\n",
    "\n",
    "        #Quantização\n",
    "        if n_bits > 0 and alpha > 0:\n",
    "                for i, layer_weights in enumerate(model.trainable_variables):\n",
    "                    if 'bn' in layer_weights.name:\n",
    "                        pass\n",
    "                    else:\n",
    "                        qk_line = (tf.reduce_max(tf.math.abs(layer_weights)) - bk[i]) / (2 ** (n_bits - 1) - 1)\n",
    "                        ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
    "                        layer_weights.assign(ck)\n",
    "\n",
    "        bk.clear()\n",
    "\n",
    "        #Test\n",
    "        acc_val = np.array([])\n",
    "        for step, (x_batch_test, y_batch_test) in enumerate(test_ds):\n",
    "            test_pred = model(x_batch_test, training=False)\n",
    "            test_loss = loss_fn(y_batch_test,test_pred)\n",
    "            test_prediction = tf.argmax(test_pred, axis=1, output_type=tf.int32)\n",
    "            #test_acc = test_accuracy(y_batch_test, test_prediction)\n",
    "\n",
    "            #acc_val = np.append(acc_val,float(test_acc))\n",
    "\n",
    "            loss_test_batch = np.append(loss_test_batch,test_loss)\n",
    "            test_accuracy.update_state(y_batch_test,test_prediction)\n",
    "\n",
    "        test_acc = test_accuracy.result()\n",
    "        test_accuracy.reset_states()\n",
    "\n",
    "        model_test_acc = np.append(model_test_acc, test_acc*100)\n",
    "        model_test_loss = np.append(model_test_loss,np.mean(loss_test_batch))\n",
    "\n",
    "        print(\"Epoch {}/{} \\t Loss = {:.3f} \\t Train Acc = {:.3f}% \\t Sparsity = {:.3f}% \\t Test Loss = {:.3f} \\tTest Acc = {:.3f}%\".format(epoch+1,epochs,float(np.mean(loss_batch)),float(acc*100),sparsity, float(np.mean(loss_test_batch)),float(test_acc*100)))\n",
    "\n",
    "        if last_test_acc < test_acc:\n",
    "            print('New test accuracy is {:.3f}%, Model saved'.format(test_acc*100))\n",
    "            last_test_acc = test_acc\n",
    "            model.save(model_name)\n",
    "\n",
    "\n",
    "\n",
    "#data = {'train accuracy': model_train_acc, 'test accuracy': model_test_acc, 'sparsity': model_sparsity, 'train loss': model_train_loss, 'test loss': model_test_loss}\n",
    "\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "#df.to_csv('test_5.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1JrifP4iwB7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pLmnLcziv_3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chM2GQsLiv9q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L7LZGE4iv7n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnZ6a_rwiv5f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xpSgBBDiv3H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7DHBdFkiv03"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7K8r4zdivyg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxPFfeiGivwd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKm5233_ivuG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kG5KsADiub9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTrERcL46lkc"
   },
   "outputs": [],
   "source": [
    "#compile o modelo\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9ftm8EFs6pXL",
    "outputId": "c5cbb42b-e246-4768-dde4-a3c3fae3f2e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1563/1563 [==============================] - 63s 38ms/step - loss: 2.2691 - accuracy: 0.1921 - val_loss: 2.3930 - val_accuracy: 0.2068\n",
      "Epoch 2/200\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 1.8388 - accuracy: 0.3113 - val_loss: 1.9568 - val_accuracy: 0.3107\n",
      "Epoch 3/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 1.6531 - accuracy: 0.3801 - val_loss: 1.6128 - val_accuracy: 0.4075\n",
      "Epoch 4/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 1.5259 - accuracy: 0.4340 - val_loss: 1.7905 - val_accuracy: 0.3737\n",
      "Epoch 5/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 1.4201 - accuracy: 0.4742 - val_loss: 1.7321 - val_accuracy: 0.4026\n",
      "Epoch 6/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 1.3342 - accuracy: 0.5121 - val_loss: 1.3988 - val_accuracy: 0.5065\n",
      "Epoch 7/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 1.2571 - accuracy: 0.5429 - val_loss: 1.5462 - val_accuracy: 0.4695\n",
      "Epoch 8/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 1.1952 - accuracy: 0.5692 - val_loss: 1.2655 - val_accuracy: 0.5483\n",
      "Epoch 9/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 1.1383 - accuracy: 0.5910 - val_loss: 1.3964 - val_accuracy: 0.5144\n",
      "Epoch 10/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 1.0908 - accuracy: 0.6073 - val_loss: 1.1914 - val_accuracy: 0.5833\n",
      "Epoch 11/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 1.0534 - accuracy: 0.6236 - val_loss: 1.2776 - val_accuracy: 0.5655\n",
      "Epoch 12/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 1.0114 - accuracy: 0.6403 - val_loss: 1.7020 - val_accuracy: 0.4682\n",
      "Epoch 13/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.9786 - accuracy: 0.6530 - val_loss: 1.0752 - val_accuracy: 0.6338\n",
      "Epoch 14/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.9431 - accuracy: 0.6627 - val_loss: 1.0521 - val_accuracy: 0.6370\n",
      "Epoch 15/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.9068 - accuracy: 0.6785 - val_loss: 0.9660 - val_accuracy: 0.6659\n",
      "Epoch 16/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.8792 - accuracy: 0.6879 - val_loss: 1.1098 - val_accuracy: 0.6261\n",
      "Epoch 17/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.8546 - accuracy: 0.6977 - val_loss: 0.9737 - val_accuracy: 0.6680\n",
      "Epoch 18/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.8266 - accuracy: 0.7083 - val_loss: 0.9670 - val_accuracy: 0.6719\n",
      "Epoch 19/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.8074 - accuracy: 0.7171 - val_loss: 0.9002 - val_accuracy: 0.6945\n",
      "Epoch 20/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.7757 - accuracy: 0.7280 - val_loss: 0.8929 - val_accuracy: 0.6965\n",
      "Epoch 21/200\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 0.7595 - accuracy: 0.7331 - val_loss: 1.0647 - val_accuracy: 0.6459\n",
      "Epoch 22/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.7301 - accuracy: 0.7460 - val_loss: 0.8737 - val_accuracy: 0.7068\n",
      "Epoch 23/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.7169 - accuracy: 0.7492 - val_loss: 0.8608 - val_accuracy: 0.7083\n",
      "Epoch 24/200\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 0.6912 - accuracy: 0.7589 - val_loss: 0.7578 - val_accuracy: 0.7447\n",
      "Epoch 25/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.6747 - accuracy: 0.7638 - val_loss: 0.7750 - val_accuracy: 0.7437\n",
      "Epoch 26/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.6617 - accuracy: 0.7692 - val_loss: 0.7321 - val_accuracy: 0.7570\n",
      "Epoch 27/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.6413 - accuracy: 0.7780 - val_loss: 0.8810 - val_accuracy: 0.7108\n",
      "Epoch 28/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.6263 - accuracy: 0.7821 - val_loss: 0.7459 - val_accuracy: 0.7545\n",
      "Epoch 29/200\n",
      "1563/1563 [==============================] - 60s 39ms/step - loss: 0.6149 - accuracy: 0.7854 - val_loss: 0.7239 - val_accuracy: 0.7589\n",
      "Epoch 30/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.5984 - accuracy: 0.7922 - val_loss: 0.7367 - val_accuracy: 0.7615\n",
      "Epoch 31/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.5800 - accuracy: 0.7977 - val_loss: 0.6610 - val_accuracy: 0.7794\n",
      "Epoch 32/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.5700 - accuracy: 0.8020 - val_loss: 0.6863 - val_accuracy: 0.7755\n",
      "Epoch 33/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.5563 - accuracy: 0.8066 - val_loss: 0.7077 - val_accuracy: 0.7699\n",
      "Epoch 34/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.5399 - accuracy: 0.8127 - val_loss: 0.7314 - val_accuracy: 0.7613\n",
      "Epoch 35/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.5252 - accuracy: 0.8188 - val_loss: 0.6989 - val_accuracy: 0.7739\n",
      "Epoch 36/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.5134 - accuracy: 0.8223 - val_loss: 0.7203 - val_accuracy: 0.7726\n",
      "Epoch 37/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.5042 - accuracy: 0.8247 - val_loss: 0.6510 - val_accuracy: 0.7940\n",
      "Epoch 38/200\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 0.4917 - accuracy: 0.8305 - val_loss: 0.5791 - val_accuracy: 0.8143\n",
      "Epoch 39/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.4757 - accuracy: 0.8334 - val_loss: 0.6288 - val_accuracy: 0.8012\n",
      "Epoch 40/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.4643 - accuracy: 0.8387 - val_loss: 0.6250 - val_accuracy: 0.7970\n",
      "Epoch 41/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.4577 - accuracy: 0.8400 - val_loss: 0.6341 - val_accuracy: 0.7955\n",
      "Epoch 42/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.4415 - accuracy: 0.8469 - val_loss: 0.5560 - val_accuracy: 0.8232\n",
      "Epoch 43/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.4299 - accuracy: 0.8511 - val_loss: 0.6810 - val_accuracy: 0.7916\n",
      "Epoch 44/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.4223 - accuracy: 0.8549 - val_loss: 0.5736 - val_accuracy: 0.8208\n",
      "Epoch 45/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.4142 - accuracy: 0.8569 - val_loss: 0.5955 - val_accuracy: 0.8118\n",
      "Epoch 46/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.4057 - accuracy: 0.8585 - val_loss: 0.6188 - val_accuracy: 0.8100\n",
      "Epoch 47/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.3900 - accuracy: 0.8644 - val_loss: 0.6109 - val_accuracy: 0.8126\n",
      "Epoch 48/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.3814 - accuracy: 0.8660 - val_loss: 0.5950 - val_accuracy: 0.8175\n",
      "Epoch 49/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.3784 - accuracy: 0.8681 - val_loss: 0.6351 - val_accuracy: 0.8072\n",
      "Epoch 50/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.3628 - accuracy: 0.8736 - val_loss: 0.6016 - val_accuracy: 0.8183\n",
      "Epoch 51/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.3551 - accuracy: 0.8760 - val_loss: 0.7062 - val_accuracy: 0.7901\n",
      "Epoch 52/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.3421 - accuracy: 0.8815 - val_loss: 0.6212 - val_accuracy: 0.8111\n",
      "Epoch 53/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.3355 - accuracy: 0.8828 - val_loss: 0.6098 - val_accuracy: 0.8171\n",
      "Epoch 54/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.3272 - accuracy: 0.8857 - val_loss: 0.5988 - val_accuracy: 0.8215\n",
      "Epoch 55/200\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 0.3150 - accuracy: 0.8894 - val_loss: 0.5707 - val_accuracy: 0.8303\n",
      "Epoch 56/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.3130 - accuracy: 0.8902 - val_loss: 0.5568 - val_accuracy: 0.8331\n",
      "Epoch 57/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.3039 - accuracy: 0.8934 - val_loss: 0.5701 - val_accuracy: 0.8302\n",
      "Epoch 58/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.2986 - accuracy: 0.8956 - val_loss: 0.6366 - val_accuracy: 0.8195\n",
      "Epoch 59/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.2820 - accuracy: 0.9011 - val_loss: 0.6221 - val_accuracy: 0.8244\n",
      "Epoch 60/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.2793 - accuracy: 0.9025 - val_loss: 0.6063 - val_accuracy: 0.8262\n",
      "Epoch 61/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.2684 - accuracy: 0.9076 - val_loss: 0.5828 - val_accuracy: 0.8339\n",
      "Epoch 62/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.2629 - accuracy: 0.9080 - val_loss: 0.6814 - val_accuracy: 0.8112\n",
      "Epoch 63/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.2558 - accuracy: 0.9113 - val_loss: 0.5770 - val_accuracy: 0.8369\n",
      "Epoch 64/200\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 0.2469 - accuracy: 0.9139 - val_loss: 0.5942 - val_accuracy: 0.8305\n",
      "Epoch 65/200\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 0.2421 - accuracy: 0.9143 - val_loss: 0.5940 - val_accuracy: 0.8339\n",
      "Epoch 66/200\n",
      "  47/1563 [..............................] - ETA: 54s - loss: 0.2021 - accuracy: 0.9229"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-108d6b181dd3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#treine o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         ):\n\u001b[1;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#treine o modelo\n",
    "model.fit(x_train, y_train, epochs=200, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s80Ct_TA6sur",
    "outputId": "0867ba36-b99c-4b82-d4ee-264d393bee78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6067 - accuracy: 0.8289\n"
     ]
    }
   ],
   "source": [
    "y = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Du10ZWGiejmz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
